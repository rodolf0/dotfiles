#!/usr/bin/env python3

# All credits and glory go to https://code.google.com/p/crush-tools
# just keeping this python portable version handy for small inputs
# inspiration from https://github.com/sharkdp/shell-functools

import itertools
import sys
import os
import re
import csv
import math  # import it for eval
from collections import defaultdict
from datetime import datetime, timezone
from zoneinfo import ZoneInfo


class Cutter:
    def __init__(self, args, header, istream):
        self.fields = parse_keyspec(args.fields, header)
        self.istream = \
            itertools.chain([header], istream) if header else istream

    def __iter__(self):
        yield from (tuple(li[f] if len(li) > f else ""
                          for f in self.fields) for li in self.istream)


class DateConverter:
    AUTO_FORMATS = [
        "%B %d, %Y at %I:%M%p",
        "%a %b %d %H:%M:%S %Z %Y",
        "%s",
    ]

    def __init__(self, args, header, istream):
        self.fields = parse_keyspec(args.fields, header)
        self.datefields = set(self.fields)
        self.header = header
        self.istream = istream
        self.outfmt = args.ofmt

        # parse input as if it were this timezone
        tz_input = ZoneInfo(args.itz) if args.itz else None
        # if input is timezone-aware, then format output in local-tz by default
        tz_output = datetime.now(
            timezone.utc).astimezone().tzinfo if tz_input else None
        # override default output-tz if we want something else than local
        tz_output = ZoneInfo(args.otz) if args.otz else tz_output
        self.tz_input, self.tz_output = tz_input, tz_output

    # try multiple input formats
    def _tryparse(self, input_tm):
        for fmt in DateConverter.AUTO_FORMATS:
            try:
                # ugly hack to parse timestamp
                tm = (datetime.fromtimestamp(int(input_tm)) if fmt == "%s"
                      else datetime.strptime(input_tm, fmt))
                if self.tz_input:
                    tm = tm.replace(tzinfo=self.tz_input)
                if self.tz_output:
                    tm = tm.astimezone(self.tz_output)
                return tm.strftime(self.outfmt)
            except ValueError:
                pass
        return "strptime-err"

    def __iter__(self):
        if self.header:
            yield self.header
        yield from (tuple(self._tryparse(f) if i in self.datefields else f
                    for i, f in enumerate(li)) for li in self.istream)


class Aggregator:
    def __init__(self, args, header, istream):
        self.header = header or ["col%d" % i for i in range(1, 50)]
        self.istream = istream
        self.keys = parse_keyspec(args.keys, header) if args.keys else []
        self.pivots = parse_keyspec(args.pivots, header) if args.pivots else []
        self.aggregations = self._parse_aggregations(args, header)
        # list of all columns we need aggregations for
        self.aggfields = set.union(*[set(f)
                                   for f in self.aggregations.values()])

    def _parse_aggregations(self, args, header):
        # Parse which aggregation to build
        aggregations = {
            'avg': parse_keyspec(args.avg, header),
            'count': [0] if args.count else [],
            'max': parse_keyspec(args.max, header),
            'min': parse_keyspec(args.min, header),
            'sum': parse_keyspec(args.sum, header),
            'uniq': parse_keyspec(args.uniq, header),
        }
        if args.pct:
            for px in args.pct:
                px, columns = px.split(":")
                aggregations['p%s' % px] = parse_keyspec(columns, header)
        return aggregations

    def _collect(self):
        key_values, pivot_values, = set(), set()
        groups = defaultdict(list)
        # aggregate lines
        for line in self.istream:
            key = tuple(line[k] for k in self.keys)
            pivot = tuple(line[p] for p in self.pivots)
            key_values.add(key)
            pivot_values.add(pivot)
            for a in self.aggfields:
                groups[(key, pivot, a)].append(line[a])
        return sorted(key_values), sorted(pivot_values), groups

    @staticmethod
    def _do_agg(fn, values):
        if len(values) == 0:
            return '-'
        if fn == 'avg':
            return "%.12g" % (sum(map(float, values)) / len(values))
        if fn == 'count':
            return str(len(values))
        if fn == 'uniq':
            return str(len(set(values)))
        if fn == 'max':
            return "%.12g" % max(map(float, values))
        if fn == 'min':
            return "%.12g" % min(map(float, values))
        if fn == 'sum':
            return "%.12g" % sum(map(float, values))
        if fn[0] == 'p':
            pct = float(fn[1:])
            values = sorted(map(float, values))
            return "%.12g" % values[int(len(values) * pct/100.0 - 0.5)]

    def _columns(self, pivot_values):
        # pivot columns: combination of all observed values for all pivots
        pivot_templates = tuple(
            "+".join("{pivot}={val}".format(pivot=self.header[piv], val=pv[i])
                     for i, piv in enumerate(self.pivots))
            for pv in pivot_values)
        # for each pivot column: build the aggregation columns we need
        columns = tuple("{aggtype}{aggcol}{pivinfo}".format(
            aggtype=aggtype,
            aggcol=("-" + self.header[af] if len(aggfields) > 1 else ''),
            pivinfo=('[{}]'.format(pt) if pt else ''))
            for aggtype, aggfields in self.aggregations.items()
            for af in aggfields
            for pt in pivot_templates)
        return columns

    def __iter__(self):
        key_values, pivot_values, groups = self._collect()
        # yield the header
        columns = self._columns(pivot_values)
        yield tuple(self.header[k] for k in self.keys) + columns
        # write aggregations
        for kf in key_values:
            yield kf + tuple(self._do_agg(fn, groups[(kf, pf, af)])
                             for fn, aggcols in self.aggregations.items()
                             for af in aggcols
                             for pf in pivot_values)


class Evaler:
    "Calculate new columns using python, calcspec: [(expr, header), ...]"

    def __init__(self, args, header, istream):
        self.header = header
        self.istream = istream
        argre = re.compile(r"(?:{[^}]+})")
        # for each expression we need to build a lambda that calculates it
        self.exprs = [self._compile_expr(argre, e, header) for e in args.exprs]

    @staticmethod
    def _compile_expr(field_regex, expr, header):
        # find all column references within expr and resolve to indexes
        expr_refs = set(field_regex.findall(expr))
        expr_fields = ",".join(f.strip("{}") for f in expr_refs)
        expr_fields = parse_keyspec(expr_fields, header)
        # make anonymous variables for each column reference
        varnames = ["__x%d" % i for i in expr_fields]
        # re-write expression to use variables
        for field, var in zip(expr_refs, varnames):
            expr = expr.replace(field, var)
        # evaluate expression into a function
        try:
            expr = eval("lambda %s: %s" % (", ".join(varnames), expr))
        except SyntaxError as ex:
            print(f"failed to parse: {expr}. Ex: {ex}", file=sys.stderr)
            sys.exit(1)
        return (expr, expr_fields)


class Calculator(Evaler):
    "Calculate fields by python expression, calcspec: [expr, ...]"

    def __init__(self, args, header, istream):
        super().__init__(args, header, istream)
        self.filter = args.filter
        # build header labels
        self.eheaders = args.headers.split(",") if args.headers else args.exprs
        assert args.headers is None or len(self.eheaders) == len(args.exprs)

    def __iter__(self):
        if self.header:
            yield self.header + ([] if self.filter else self.eheaders)
        for line in self.istream:
            exprs = [fn(*[line[i] for i in args]) for (fn, args) in self.exprs]
            if not self.filter:
                yield line + [str(x) for x in exprs]
            elif all(exprs):
                yield line


class Sorter(Evaler):
    "Sort input by python expression, sortspec: [expr, ...]"

    def __init__(self, args, header, istream):
        super().__init__(args, header, istream)
        self.reverse = args.reverse

    def __iter__(self):
        if self.header:
            yield self.header
        yield from sorted(
            self.istream, reverse=self.reverse,
            key=lambda line:
            tuple(fn(*[line[i] for i in args]) for fn, args in self.exprs))


def splitstream(istream, splitspec, bucketfn, outpath, outpat, header, wrmaker):
    "Split input stream into different files"
    wrote_headers = set()

    def _flush(bucket, stream):
        outfile = os.path.join(outpath, outpat.replace("%", bucket))
        with open(outfile, "a") as out:
            if header and bucket not in wrote_headers:
                wrote_headers.add(bucket)
                stream = itertools.chain([header], stream)
            wrmaker(out)(stream)

    ostreams = defaultdict(list)
    for line in istream:
        # bucket function is called with 'splitspec' fields
        # eg: -f 2,3  --->  lambda x, y: <bucket-logic>
        b = bucketfn(*tuple(line[arg] for arg in splitspec))
        stream = ostreams[b]
        stream.append(line)
        # flush to file if we've got enough rows
        if len(stream) > 255:
            _flush(b, stream)
            del ostreams[b]
    # flush pending data
    for b, stream in ostreams.items():
        _flush(b, stream)


def parse_keyspec(keyspec, header):
    "Expand keyspec into a list of fields, header are for named fields"
    if not keyspec:
        return []
    idxs = []
    for spec in keyspec.split(","):
        if header:
            if spec in header:
                idxs.append(header.index(spec))
            else:
                print("unknown field: " + spec, file=sys.stderr)
                sys.exit(1)
        elif "-" in spec:
            start, _, end = spec.partition("-")
            start, end = int(start), int(end)
            idxs.extend(range(start - 1, end))
        else:
            idxs.append(int(spec) - 1)
    return idxs


def build_reader(args):
    "Check how to interpret input stream"
    istream = None
    # Figure out how to split the input stream
    if args.csv:
        istream = csv.reader(args.infile)
    elif len(args.delim) == 1:
        istream = (li.strip().split(args.delim) for li in args.infile)
    else:
        _delim = re.compile(args.delim)
        istream = (_delim.split(li.strip()) for li in args.infile)
    # TODO: limit the amount of splitting based on header?
    header = next(istream) if args.header else None
    return header, istream


def build_writer(args, ostream):
    "Return a function to write results"
    if args.ocsv:
        def _csv_writer(istream):
            csv.writer(ostream).writerows(istream)
            ostream.flush()
        return _csv_writer
    elif args.odelim:
        def _odelim_writer(istream):
            ostream.writelines(args.odelim.join(li) + "\n" for li in istream)
            ostream.flush()
        return _odelim_writer

    # fixed-width column writer
    def _fixed_width_writer(istream):
        istream = iter(istream)  # treat lists and generators equally
        normalized = list(itertools.islice(istream, 128))
        istream = itertools.chain(normalized, istream)
        widths = ({i: len(f) for i, f in enumerate(normalized[0])}
                  if len(normalized) > 0 else {})
        # Don't print alignment if single column
        if len(widths) == 1:
            ostream.writelines(li[0] + "\n" for li in istream)
            ostream.flush()
            return
        for line in normalized:
            widths = {
                i: max(widths.get(i, 0), len(col)) for i, col in enumerate(line)
            }
        outfmt = '  '.join("{:<%d}" % w for w in widths.values()) + "\n"
        ostream.writelines(outfmt.format(*li) for li in istream)
        ostream.flush()
    return _fixed_width_writer


# cmdline parsing
def main():
    import argparse
    gp = argparse.ArgumentParser()
    gp.add_argument("--delim", "-d", default=r"\s+")
    gp.add_argument("--odelim", "-o", help="output field delimiter")
    gp.add_argument("--csv", action="store_true")
    gp.add_argument("--ocsv", action="store_true")
    gp.add_argument("--header", "-H", action="store_true")
    sp = gp.add_subparsers(dest="command")

    p = sp.add_parser("cut")
    p.add_argument("--fields", "-f", required=True)
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)

    p = sp.add_parser("tm")
    p.add_argument("--fields", "-f", required=True)
    p.add_argument("--ofmt", "-o", default="%F %T%z",
                   help="date output format")
    p.add_argument("--itz", help="input timezone")
    p.add_argument("--otz", help="output timezone")
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)

    p = sp.add_parser("agg")
    p.add_argument("--keys", "-k", metavar="{columns}")
    p.add_argument("--pivots", "-p", metavar="{columns}")
    p.add_argument("--count", action="store_const", const='1')
    p.add_argument("--pct", metavar=('{pX}:{columns},...'), nargs='+')
    for op in ['sum', 'avg', 'min', 'max', 'uniq']:  # 'mode', 'gmean', 'var'
        p.add_argument("--%s" % op, metavar="{columns}")
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)

    p = sp.add_parser("calc")
    p.add_argument("--headers", "-H")
    p.add_argument("--filter", "-f", action="store_true")
    p.add_argument("exprs", nargs="+")
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)

    p = sp.add_parser("sortby")
    p.add_argument("--reverse", "-r", action="store_true")
    p.add_argument("exprs", nargs="+")
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)

    p = sp.add_parser("split")
    p.add_argument("--fields", "-f", required=True,
                   help="arguments to the partition function")
    p.add_argument("--bucket", "-b", default="lambda x: str(x)",
                   help="bucket function eg: lambda x: str(abs(hash(x)) % 10)")
    p.add_argument("--path", "-p", default=".")
    p.add_argument("--outpat", "-o", default="%",
                   help="Output filename pattern, %% is the bucket name")
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)

    args = gp.parse_args()
    if not args.command:
        return gp.print_help()

    # header: to address columns by name we need to say so explicitly
    # choose how we consume input / write output
    header, istream = build_reader(args)
    owriter = build_writer(args, sys.stdout)

    # dispatch subcommand
    if args.command == "cut":
        owriter(Cutter(args, header, istream))

    if args.command == "tm":
        owriter(DateConverter(args, header, istream))

    elif args.command == "agg":
        owriter(Aggregator(args, header, istream))

    elif args.command == "calc":
        owriter(Calculator(args, header, istream))

    elif args.command == "sortby":
        owriter(Sorter(args, header, istream))

    elif args.command == "split":
        splitspec = parse_keyspec(args.fields, header)
        try:
            bucketfn = eval(args.bucket)
        except SyntaxError as ex:
            print(f"failed to parse: {args.bucket}. Ex: {ex}", file=sys.stderr)
            sys.exit(1)
        splitstream(istream, splitspec, bucketfn,
                    args.path, args.outpat, header,
                    lambda ostream: build_writer(args, ostream))


if __name__ == "__main__":
    try:
        main()
    except IOError as e:
        import errno
        if e.errno == errno.EPIPE:
            pass
    sys.stderr.close()
    sys.stdout.close()
