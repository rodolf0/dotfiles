#!/usr/bin/env python3

# All credits and glory go to https://code.google.com/p/crush-tools
# just keeping this python portable version handy for small inputs
# inspiration from https://github.com/sharkdp/shell-functools

import itertools
import sys, os, re, csv, math
from collections import defaultdict
from datetime import datetime


class Cutter:
    def __init__(self, args, header, istream):
        self.fields = parse_keyspec(args.fields, header)
        self.istream = \
            itertools.chain([header], istream) if header else istream

    def __iter__(self):
        yield from (tuple(l[f] for f in self.fields) for l in self.istream)


class DateConverter:
    AUTO_FORMATS = [
        "%B %d, %Y at %I:%M%p",
        "%a %b %d %H:%M:%S %Z %Y",
        "%s",
    ]

    def __init__(self, args, header, istream):
        self.fields = parse_keyspec(args.fields, header)
        self.datefields = set(self.fields)
        self.header = header
        self.istream = istream
        self.outfmt = args.ofmt

        import pytz
        # parse input as if it were this timezone
        tz_input = pytz.timezone(args.itz) if args.itz else None
        # if input is timezone-aware, then format output in local-tz by default
        tz_output = datetime.utcnow().astimezone().tzinfo if tz_input else None
        # override default output-tz if we want something else than local
        tz_output = pytz.timezone(args.otz) if args.otz else tz_output
        self.tz_input, self.tz_output = tz_input, tz_output

    # try multiple input formats
    def _tryparse(self, input_tm):
        for fmt in DateConverter.AUTO_FORMATS:
            try:
                # ugly hack to parse timestamp
                tm = (datetime.fromtimestamp(int(input_tm)) if fmt == "%s"
                      else datetime.strptime(input_tm, fmt))
                if self.tz_input:
                    tm = tm.replace(tzinfo=self.tz_input)
                if self.tz_output:
                    tm = tm.astimezone(self.tz_output)
                return tm.strftime(self.outfmt)
            except ValueError:
                pass
        return "strptime-err"

    def __iter__(self):
        if self.header:
            yield self.header
        yield from (tuple(self._tryparse(f) if i in self.datefields else f
                    for i, f in enumerate(l)) for l in self.istream)


class Aggregator:
    def __init__(self, args, header, istream):
        self.header = header or ["col%d" % i for i in range(1, 50)]
        self.istream = istream
        self.keys = parse_keyspec(args.keys, header) if args.keys else []
        self.pivots = parse_keyspec(args.pivots, header) if args.pivots else []
        self.aggregations = self._parse_aggregations(args, header)
        self.aggfields = set.union(*[set(f) for f in self.aggregations.values()])

    def _parse_aggregations(self, args, header):
        # Parse which aggregation to build
        aggregations = {
            'avg': parse_keyspec(args.avg, header),
            'count': [0] if args.count else [],
            'max': parse_keyspec(args.max, header),
            'min': parse_keyspec(args.min, header),
            'sum': parse_keyspec(args.sum, header),
            'uniq': parse_keyspec(args.uniq, header),
        }
        if args.pct:
            for px in args.pct:
                px, columns = px.split(":")
                aggregations['p%s' % px] = parse_keyspec(columns, header)
        return aggregations

    def _collect(self):
        all_keys, all_pivots, values = set(), set(), defaultdict(list)
        # aggregate lines
        for line in self.istream:
            key = tuple(line[k] for k in self.keys)
            pivot = tuple(line[p] for p in self.pivots)
            all_keys.add(key)
            all_pivots.add(pivot)
            for a in self.aggfields:
                values[(key, pivot, a)].append(line[a])
        return all_keys, all_pivots, values

    @staticmethod
    def _do_agg(fn, values):
        if len(values) == 0: return '-'
        if fn == 'avg': return "%.12g" % (sum(map(float, values)) / len(values))
        if fn == 'count': return str(len(values))
        if fn == 'uniq': return str(len(set(values)))
        if fn == 'max': return "%.12g" % max(map(float, values))
        if fn == 'min': return "%.12g" % min(map(float, values))
        if fn == 'sum': return "%.12g" % sum(map(float, values))
        if fn[0] == 'p':
            pct = float(fn[1:])
            values = sorted(map(float, values))
            return "%.12g" % values[int(len(values) * pct/100.0 - 0.5)]

    def __iter__(self):
        all_keys, all_pivots, values = self._collect()
        # pivot columns: combination of all observed values for all pivots
        pivcols = tuple("+".join("%s=%s" % (self.header[p], pv[i])
                                 for i, p in enumerate(self.pivots))
                        for pv in all_pivots)
        # for each pivot column: build the aggregation columns we need
        aggcols = tuple("{aggtype}{aggcol}{pivcolumn}".format(
            aggtype=agg,
            aggcol=("-" + self.header[aggcol] if agg != 'count' and len(aggcols) > 1 else ''),
            pivcolumn=('[' + pc + ']' if pc else ''))
                        for agg, aggcols in self.aggregations.items()
                        for aggcol in aggcols
                        for pc in pivcols)

        # Finaly yield the header
        yield tuple(self.header[k] for k in self.keys) + aggcols
        # write aggregations
        for k in all_keys:
            yield k + tuple(self._do_agg(fn, values[(k, p, a)])
                            for fn, aggcols in self.aggregations.items()
                            for a in aggcols
                            for p in all_pivots)


def calcfields(instream, calcspec, filter_rows, header):
    "Calculate new columns using python, calcspec: [(expr, header), ...]"
    calcspec = list(calcspec)
    if header:
        yield header + ([] if filter_rows else [h for _, h in calcspec])
    for line in instream:
        exprs = [fn(*[line[i] for i in args]) for (fn, args), _ in calcspec]
        if not filter_rows:
            yield line + [str(x) for x in exprs]
        elif all(exprs):
            yield line


def sortfields(instream, sortspec, reverse, header):
    "Sort input by python expression, sortspec: [expr, ...]"
    sortspec = list(sortspec)
    if header:
        yield header
    yield from sorted(instream, reverse=reverse, key=lambda line: tuple(
        fn(*[line[i] for i in args]) for fn, args in sortspec))


def splitstream(istream, splitspec, bucketfn, outpath, outpat, header, wrmaker):
    "Split input stream into different files"
    wrote_headers = set()

    def _flush(bucket, stream):
        outfile = os.path.join(outpath, outpat.replace("%", bucket))
        with open(outfile, "a") as out:
            if header and bucket not in wrote_headers:
                wrote_headers.add(bucket)
                stream = itertools.chain([header], stream)
            wrmaker(out)(stream)

    ostreams = defaultdict(list)
    for line in istream:
        # bucket function is called with 'splitspec' fields
        # eg: -f 2,3  --->  lambda x, y: <bucket-logic>
        b = bucketfn(*tuple(line[arg] for arg in splitspec))
        stream = ostreams[b]
        stream.append(line)
        # flush to file if we've got enough rows
        if len(stream) > 255:
            _flush(b, stream)
            del ostreams[b]
    # flush pending data
    for b, stream in ostreams.items():
        _flush(b, stream)


def parse_keyspec(keyspec, header):
    "Expand keyspec into a list of fields, header are for named fields"
    if not keyspec:
        return []
    idxs = []
    for spec in keyspec.split(","):
        if header:
            if spec in header:
                idxs.append(header.index(spec))
            else:
                print("unknown field: " + spec, file=sys.stderr)
                sys.exit(1)
        elif "-" in spec:
            start, _, end = spec.partition("-")
            start, end = int(start), int(end)
            idxs.extend(range(start - 1, end))
        else:
            idxs.append(int(spec) - 1)
    return idxs


def build_reader(args):
    "Check how to interpret input stream"
    istream = None
    # Figure out how to split the input stream
    if args.csv:
        istream = csv.reader(args.infile)
    elif len(args.delim) == 1:
        istream = (l.strip().split(args.delim) for l in args.infile)
    else:
        _delim = re.compile(args.delim)
        istream = (_delim.split(l.strip()) for l in args.infile)
    # TODO: limit the amount of splitting based on header?
    header = next(istream) if args.header else None
    return header, istream


def build_writer(args, ostream):
    "Return a function to write results"
    if args.ocsv:
        def _csv_writer(istream):
            csv.writer(ostream).writerows(istream)
            ostream.flush()
        return _csv_writer
    elif args.odelim:
        def _odelim_writer(istream):
            ostream.writelines(args.odelim.join(l) + "\n" for l in istream)
            ostream.flush()
        return _odelim_writer

    # fixed-width column writer
    def _fixed_width_writer(istream):
        istream = iter(istream)  # treat lists and generators equally
        normalized = list(itertools.islice(istream, 128))
        istream = itertools.chain(normalized, istream)
        widths = ({i: len(f) for i, f in enumerate(normalized[0])}
                  if len(normalized) > 0 else {})
        # Don't print alignment if single column
        if len(widths) == 1:
            ostream.writelines(l[0] + "\n" for l in istream)
            ostream.flush()
            return
        for line in normalized:
            widths = {
                i: max(widths.get(i, 0), len(col)) for i, col in enumerate(line)
            }
        outfmt = '  '.join("{:<%d}" % w for i, w in enumerate(widths.values())) + "\n"
        ostream.writelines(outfmt.format(*l) for l in istream)
        ostream.flush()
    return _fixed_width_writer


# cmdline parsing
def main():
    import argparse
    gp = argparse.ArgumentParser()
    gp.add_argument("--delim", "-d", default=r"\s+")
    gp.add_argument("--odelim", "-o", help="output field delimiter")
    gp.add_argument("--csv", action="store_true")
    gp.add_argument("--ocsv", action="store_true")
    gp.add_argument("--header", "-H", action="store_true")
    sp = gp.add_subparsers(dest="command")

    p = sp.add_parser("cut")
    p.add_argument("--fields", "-f", required=True)
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)

    p = sp.add_parser("tm")
    p.add_argument("--fields", "-f", required=True)
    p.add_argument("--ofmt", "-o", default="%F %T%z",
                   help="date output format")
    p.add_argument("--itz", help="input timezone")
    p.add_argument("--otz", help="output timezone")
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)

    p = sp.add_parser("agg")
    p.add_argument("--keys", "-k", metavar="{columns}")
    p.add_argument("--pivots", "-p", metavar="{columns}")
    p.add_argument("--count", action="store_const", const='1')
    p.add_argument("--pct", metavar=('{pX}:{columns},...'), nargs='+')
    for op in ['sum', 'avg', 'min', 'max', 'uniq']:  # 'mode', 'gmean', 'var'
        p.add_argument("--%s" % op, metavar="{columns}")
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)

    p = sp.add_parser("calc")
    p.add_argument("--headers", "-H")
    p.add_argument("--filter", "-f", action="store_true")
    p.add_argument("exprs", nargs="+")
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)

    p = sp.add_parser("sortby")
    p.add_argument("--reverse", "-r", action="store_true")
    p.add_argument("exprs", nargs="+")
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)

    p = sp.add_parser("split")
    p.add_argument("--fields", "-f", required=True,
                   help="arguments to the partition function")
    p.add_argument("--bucket", "-b", default="lambda x: str(x)",
                   help="bucket function eg: lambda x: str(abs(hash(x)) % 10)")
    p.add_argument("--path", "-p", default=".")
    p.add_argument("--outpat", "-o", default="%",
                   help="Output filename pattern, %% is the bucket name")
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)

    args = gp.parse_args()
    if not args.command:
        return gp.print_help()

    # header: to address columns by name we need to say so explicitly
    # choose how we consume input / write output
    header, istream = build_reader(args)
    owriter = build_writer(args, sys.stdout)

    # dispatch subcommand
    if args.command == "cut":
        owriter(Cutter(args, header, istream))

    if args.command == "tm":
        owriter(DateConverter(args, header, istream))

    elif args.command == "agg":
        owriter(Aggregator(args, header, istream))

    elif args.command == "calc":
        argre = re.compile(r"(?:{[^}]+})")
        calcfuncs = []
        # for each expression we need to build a lambda that calculates it
        for expr in args.exprs:
            # find all references to fields within the expression
            reffields = argre.findall(expr)
            # resolve field references into indexes
            fieldspec = ",".join(f.strip("{}") for f in reffields)
            fieldspec = parse_keyspec(fieldspec, header)
            # make anonymous variable for each referenced field
            varnames = ["__x%d" % i for i in fieldspec]
            # re-write expression to use variables
            for field, var in zip(reffields, varnames):
                expr = expr.replace(field, var)
            # evaluate text into a function object
            try:
                expr = eval("lambda %s: %s" % (", ".join(varnames), expr))
            except SyntaxError:
                print("failed to parse: " + expr, file=sys.stderr)
                sys.exit(1)
            calcfuncs.append((expr, fieldspec))
        exprheaders = args.headers.split(",") if args.headers else args.exprs
        assert args.headers is None or len(exprheaders) == len(args.exprs)
        calcspec = zip(calcfuncs, exprheaders)
        owriter(calcfields(istream, calcspec, args.filter, header))

    elif args.command == "sortby":
        argre = re.compile(r"(?:{[^}]+})")
        sortfuncs = []
        # for each expression we need to build a lambda that calculates it
        for expr in args.exprs:
            # find all references to fields within the expression
            reffields = argre.findall(expr)
            # resolve field references into indexes
            fieldspec = ",".join(f.strip("{}") for f in reffields)
            fieldspec = parse_keyspec(fieldspec, header)
            # make anonymous variable for each referenced field
            varnames = ["__x%d" % i for i in fieldspec]
            # re-write expression to use variables
            for field, var in zip(reffields, varnames):
                expr = expr.replace(field, var)
            # evaluate text into a function object
            try:
                expr = eval("lambda %s: %s" % (", ".join(varnames), expr))
            except SyntaxError:
                print("failed to parse: " + expr, file=sys.stderr)
                sys.exit(1)
            sortfuncs.append((expr, fieldspec))
        owriter(sortfields(istream, sortfuncs, args.reverse, header))

    elif args.command == "split":
        splitspec = parse_keyspec(args.fields, header)
        try:
            bucketfn = eval(args.bucket)
        except SyntaxError:
            print("failed to parse: " + args.bucket, file=sys.stderr)
            sys.exit(1)
        splitstream(istream, splitspec, bucketfn,
                    args.path, args.outpat, header,
                    lambda ostream: build_writer(args, ostream))


if __name__ == "__main__":
    try:
        main()
    except IOError as e:
        import errno
        if e.errno == errno.EPIPE:
            pass
    sys.stderr.close()
    sys.stdout.close()
