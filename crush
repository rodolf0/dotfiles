#!/usr/bin/env python

# All credits and glory go to https://code.google.com/p/crush-tools
# just keeping this python portable version handy for small inputs

import itertools
import os
import re
import sys


def reorder(instream, fields):
    "Select which fields from input stream are copied to output"
    for inline in instream:
        yield list(inline[f] for f in fields)


def add_field(instream, addspecs, show_header):
    "Add fields to stream, addspec: [(insert-idx, value, header)...]"
    addspecs = list(addspecs)
    if show_header:
        line = next(instream)
        for idx, _, hdr in addspecs:
            line.insert(idx, hdr)
        yield line
    for line in instream:
        for idx, val, _ in addspecs:
            line.insert(idx, val)
        yield line


def grepfield(instream, grepspec, show_header, anyfield, icase, invert):
    "Filter stream by fields matching regex, grepspec: [(field, pattern)...]"
    flags = re.I if icase else 0
    regexes = tuple((idx, re.compile(pat, flags)) for idx, pat in grepspec)
    if show_header:
        yield next(instream)
    fn = any if anyfield else all
    if invert:
        for line in instream:
            if fn(not regex.search(line[idx]) for idx, regex in regexes):
                yield line
    else:
        for line in instream:
            if fn(regex.search(line[idx]) for idx, regex in regexes):
                yield line


def calcfields(instream, calcspec, show_header):
    "Calculate new columns using python, calcspec: [(expr, header)...]"
    calcspec = list(calcspec)
    if show_header:
        line = next(instream)
        line.extend(header for _, header in calcspec)
        yield line
    for line in instream:
        line.extend(str(fn(*[line[i] for i in args]))
                    for (fn, args), _ in calcspec)
        yield line


def PercentileAgg(pct, col):
    class Aggregator(object):
        __slots__ = ["aggr"]
        percentile = pct
        column = col

        def __init__(self):
            self.aggr = []

        def __call__(self, line):
            self.aggr.append(float(line[self.column]))

        def __str__(self):
            self.aggr.sort()
            idx = int(len(self.aggr) * self.percentile / 100.0)
            return "%.12g" % self.aggr[idx]

        @classmethod
        def header(cls, header):
            return "p%g-%s" % (cls.percentile, header[cls.column])
    return Aggregator


def AdderAgg(col):
    class Aggregator(object):
        __slots__ = ["aggr"]
        column = col

        def __init__(self):
            self.aggr = 0.0

        def __call__(self, line):
            self.aggr += float(line[self.column])

        def __str__(self):
            return "%.12g" % self.aggr

        @classmethod
        def header(cls, header):
            return "Sum-%s" % header[cls.column]
    return Aggregator


class CounterAgg(object):
    __slots__ = ["aggr"]

    def __init__(self):
        self.aggr = 0

    def __call__(self, line):
        self.aggr += 1

    def __str__(self):
        return "%g" % self.aggr

    @classmethod
    def header(cls, header):
        return "Count"


def aggregate(instream, keys, pivots, show_header, aggregators):
    "Aggregate values by keys, optionally break down by pivots"
    header = next(instream) if show_header else None
    aggs, all_keys, all_pivs = {}, set(), set()
    for inline in instream:
        key = tuple(inline[f] for f in keys)
        piv = tuple(inline[f] for f in pivots)
        all_keys.add(key)
        all_pivs.add(piv)
        agg = aggs.setdefault(key, {})
        # initialize a list of aggregators for each new key/pivot
        agg = agg.setdefault(piv, tuple(a() for a in aggregators))
        for aggregator in agg:
            aggregator(inline)  # aggregate line

    all_keys, all_pivs = sorted(all_keys), sorted(all_pivs)
    # pivot columns: combination of all observed values for all pivots
    pivcolumns = tuple("+".join("%s=%s" % (header[p], pv[i])
                                for i, p in enumerate(pivots))
                       for pv in all_pivs)
    if show_header:
        yield tuple(header[k] for k in keys) + \
            tuple("%s:%s" % (pivcol, a.header(header))
                  for pivcol in pivcolumns for a in aggregators)
    emptyaggs = ['-'] * len(aggregators)
    for k in all_keys:
        yield k + tuple(str(a) for p in all_pivs
                        for a in aggs.get(k, {}).get(p, emptyaggs))


def stream_writer(instream, odelim, outstream):
    outstream.writelines(odelim.join(l) + "\n" for l in instream)


def column_writer(instream, odelim, outstream, norm_buffer=15):
    # inspect first lines to estimate column widths
    normlines = list(itertools.islice(instream, norm_buffer))
    widths = [len(col) for col in normlines[0]] if len(normlines) > 0 else []
    for line in normlines:
        for colidx, value in enumerate(line):
            widths[colidx] = max(widths[colidx], len(value))
    # format output in columns with learnt widths
    instream = itertools.chain(normlines, instream)
    outfmt = odelim.join("{:<%d}" % w for w in widths) + "\n"
    outstream.writelines(outfmt.format(*l) for l in instream)


def split_writer(instream, odelim, fieldspec, header, bucket, outpath, outpat):
    "Split input stream into different files"
    ostreams, wrote_headers = {}, set()
    try:
        for line in instream:
            # bucket function is called with selected fields
            # eg: -f 2,3  --->  lambda x, y: <bucket-logic>
            b = bucket(*tuple(line[arg] for arg in fieldspec))
            # check if we need to open the output stream
            if b not in ostreams:
                filename = outpat.replace("%", b)
                outfile = os.path.join(outpath, filename)
                try:
                    ostreams[b] = open(outfile, "a")
                except IOError as e:
                    if e.errno != 24:  # too many open files
                        raise
                    for f in ostreams.values():
                        f.close()
                    ostreams = {b: open(outfile, "a")}
                # only write header once to each file even if reopening
                if header and b not in wrote_headers:
                    wrote_headers.add(b)
                    ostreams[b].write(odelim.join(header) + "\n")
            ostreams[b].write(odelim.join(line) + "\n")
    finally:
        for f in ostreams.values():
            f.close()


# resolve field references into column indexes
# also try to figure out if first-line is a header or data
def parse_keyspec(keyspec, maybeheader):
    idxs, detected_header = [], False
    for spec in keyspec.split(","):
        if spec in maybeheader:
            idxs.append(maybeheader.index(spec))
            detected_header = True
        elif "-" in spec:
            start, _, end = spec.partition("-")
            start, end = int(start or 1), int(end or len(maybeheader))
            idxs.extend(range(start - 1, end))
        else:
            idxs.append(int(spec) - 1)
    assert all(0 <= k < len(maybeheader) for k in idxs)
    return idxs, detected_header


# cmdline parsing
def main():
    import argparse
    gp = argparse.ArgumentParser()
    gp.add_argument("--delim", "-d", default=re.compile(r"\s+"),
                    help="regex to split input fields")
    gp.add_argument("--odelim", "-o", default="\t",
                    help="output field delimiter")
    gp.add_argument("--no-col-normalize", "-C", action="store_true",
                    help="don't normalize output column width")
    gp.add_argument("--no-header", "-N", action="store_true",
                    help="first line is data")
    sp = gp.add_subparsers(dest="command")

    p = sp.add_parser("cut")
    p.add_argument("--fields", "-f", required=True)
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)

    p = sp.add_parser("addfield")
    p.add_argument("--idxs", "-i", required=True)
    p.add_argument("--values", "-v", required=True)
    p.add_argument("--headers", "-H")
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)

    p = sp.add_parser("grep")
    p.add_argument("--invert", "-v", action="store_true")
    p.add_argument("--icase", "-i", action="store_true")
    p.add_argument("--anyfield", "-a", action="store_true")
    p.add_argument("--fields", "-f", required=True)
    p.add_argument("patterns", nargs="+")
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)

    p = sp.add_parser("split")
    p.add_argument("--fields", "-f", required=True,
                   help="arguments to the partition function")
    p.add_argument("--bucket", "-b",
                   default="lambda x: str(abs(hash(x)) % 10)",
                   help="Partition function, eg lambda x, y: x+y")
    p.add_argument("--path", "-p", default=".")
    p.add_argument("--outpat", "-o", default="%",
                   help="Output filename pattern, %% is the bucket name")
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)

    p = sp.add_parser("calc")
    p.add_argument("--headers", "-H")
    p.add_argument("exprs", nargs="+")
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)

    p = sp.add_parser("aggr")
    p.add_argument("--headers", "-H")
    p.add_argument("--sdelim", default="|",
                   help="output field sub-delimiter")
    p.add_argument("--keys", "-k")
    p.add_argument("--pivots", "-p")
    p.add_argument("infile", type=argparse.FileType('r'),
                   nargs="?", default=sys.stdin)
    p.add_argument("--count", action="store_true")
    p.add_argument("--p99")
    p.add_argument("--sum")

    args = gp.parse_args()
    if not args.command:
        return gp.print_help()

    out = None
    instream = (args.delim.split(l.rstrip("\r\n")) for l in args.infile)
    firstline = next(instream)
    instream = itertools.chain([firstline], instream)

    # Headers
    # a. has no input-header (or treated as data -N) -> no output-header
    # b. has input-header -> we always print it (filter out some other way)

    if args.command == "cut":
        fields, _ = parse_keyspec(args.fields, firstline)
        out = reorder(instream, fields)

    elif args.command == "addfield":
        fields, detected_header = parse_keyspec(args.idxs, firstline)
        show_header = not args.no_header and (detected_header or args.headers)
        assert args.headers is None or len(args.headers) == len(args.values)
        newheaders = args.headers if args.headers else args.values
        addspec = zip(fields, args.values.split(","), newheaders.split(","))
        out = add_field(instream, addspec, show_header)

    elif args.command == "grep":
        fields, detected_header = parse_keyspec(args.fields, firstline)
        show_header = not args.no_header and detected_header
        assert len(fields) == len(args.patterns)
        grepspec = zip(fields, args.patterns)
        out = grepfield(instream, grepspec, show_header,
                        args.anyfield, args.icase, args.invert)

    elif args.command == "split":
        fields, detected_header = parse_keyspec(args.fields, firstline)
        show_header = not args.no_header and detected_header
        # discard header from input stream and replicate it on each output file
        header = next(instream) if show_header else None
        bucketfn = eval(args.bucket)
        split_writer(instream, args.odelim, fields, header,
                     bucketfn, args.path, args.outpat)
        return  # don't use default writer

    elif args.command == "calc":
        argre = re.compile(r"({[^}]+})")
        calcfuncs, detected_header = [], False
        assert args.headers is None or len(args.headers) == len(args.exprs)
        # for each expression we need to build a lambda that calculates it
        for expr in args.exprs:
            # find all references to fields within the expression
            reffields = argre.findall(expr)
            # resolve field references into indexes
            fieldspec = ",".join(f.strip("{}") for f in reffields)
            fieldspec, detected_h = parse_keyspec(fieldspec, firstline)
            detected_header |= detected_h
            # make a variable for each referenced field
            varnames = ["_x%d" % i for i in fieldspec]
            # re-write expression to use variables
            for field, var in zip(reffields, varnames):
                expr = expr.replace(field, var)
            # evaluate text into a function object
            expr = eval("lambda %s: %s" % (", ".join(varnames), expr))
            calcfuncs.append((expr, fieldspec))
        show_header = not args.no_header and (detected_header or args.headers)
        exprheaders = args.headers.split(",") if args.headers else args.exprs
        calcspec = zip(calcfuncs, exprheaders)
        out = calcfields(instream, calcspec, show_header)

    elif args.command == "aggr":
        keys, pivots, aggregators = [], [], []
        detected_header = False
        if args.keys:
            keys, detectedh_agg = parse_keyspec(args.keys, firstline)
            detected_header |= detectedh_agg
        if args.pivots:
            pivots, detectedh_agg = parse_keyspec(args.pivots, firstline)
            detected_header |= detectedh_agg
        if args.count:
            aggregators.append(CounterAgg)
        if args.p99:
            columns, detectedh_agg = parse_keyspec(args.p99, firstline)
            aggregators.extend(PercentileAgg(99.0, c) for c in columns)
            detected_header |= detectedh_agg
        if args.sum:
            columns, detectedh_agg = parse_keyspec(args.sum, firstline)
            aggregators.extend(AdderAgg(c) for c in columns)
            detected_header |= detectedh_agg
        show_header = not args.no_header and detected_header
        out = aggregate(instream, keys, pivots, show_header, aggregators)

    elif args.command == "uniq":
        pass
    elif args.command == "merge":
        pass
    elif args.command == "sample":
        pass
    elif args.command == "convdate":
        pass

    if args.no_col_normalize:
        stream_writer(out, args.odelim, sys.stdout)
    else:
        column_writer(out, args.odelim, sys.stdout)


if __name__ == "__main__":
    sys.exit(main())
